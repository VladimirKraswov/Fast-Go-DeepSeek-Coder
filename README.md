<p align="center">
<img width="1000px" alt="DeepSeek Coder" src="pictures/logo.png">
</p>
<p align="center"><a href="https://www.deepseek.com/">[<img src="pictures/home.png" width="20px"> Homepage]</a> | <a href="https://coder.deepseek.com/">[ü§ñ Chat with DeepSeek Coder]</a> | <a href="https://huggingface.co/deepseek-ai">[ü§ó Models Download]</a> | <a href="https://discord.gg/Tc7c45Zzu5">[Discord]</a> | <a href="https://github.com/guoday/assert/blob/main/QR.png?raw=true">[WeChat (ÂæÆ‰ø°)]</a></p>
<p align="center">
  <a href="https://huggingface.co/papers/2401.14196"><b>Paper Link</b>üëÅÔ∏è</a>
</p>
<hr>


### 1. Introduction of DeepSeek Coder

DeepSeek Coder ‚Äî —ç—Ç–æ —Å–µ—Ä–∏—è –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–æ–¥–æ–º, –∫–∞–∂–¥–∞—è –∏–∑ –∫–æ—Ç–æ—Ä—ã—Ö –æ–±—É—á–µ–Ω–∞ —Å –Ω—É–ª—è –Ω–∞ 2 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤. –î–∞—Ç–∞—Å–µ—Ç —Å–æ—Å—Ç–æ–∏—Ç –Ω–∞ **87% –∏–∑ –∫–æ–¥–∞** –∏ –Ω–∞ **13% –∏–∑ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ —è–∑—ã–∫–∞** (–∞–Ω–≥–ª–∏–π—Å–∫–∏–π –∏ –∫–∏—Ç–∞–π—Å–∫–∏–π).  

–ú—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ–º –º–æ–¥–µ–ª–∏ —Ä–∞–∑–Ω–æ–≥–æ —Ä–∞–∑–º–µ—Ä–∞ ‚Äî –æ—Ç **1B –¥–æ 33B** –ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –ö–∞–∂–¥–∞—è –º–æ–¥–µ–ª—å –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–∞ –Ω–∞ **–∫–æ–¥–µ –ø—Ä–æ–µ–∫—Ç–Ω–æ–≥–æ —É—Ä–æ–≤–Ω—è**, –∏—Å–ø–æ–ª—å–∑—É—è **–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ –≤ 16K —Ç–æ–∫–µ–Ω–æ–≤** –∏ –¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—É—é –∑–∞–¥–∞—á—É **"–∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤"** (fill-in-the-blank). –≠—Ç–æ –ø–æ–∑–≤–æ–ª—è–µ—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—Ç—å **–∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–¥–∞** –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–æ–µ–∫—Ç–∞ –∏ –≤—Å—Ç–∞–≤–∫—É –Ω–µ–¥–æ—Å—Ç–∞—é—â–∏—Ö —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤.  

–° —Ç–æ—á–∫–∏ –∑—Ä–µ–Ω–∏—è –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–µ–π –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è, **DeepSeek Coder –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –ø–µ—Ä–µ–¥–æ–≤—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–µ–¥–∏ open-source –º–æ–¥–µ–ª–µ–π –¥–ª—è –∫–æ–¥–∞**, –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞—è **—Ä–∞–∑–ª–∏—á–Ω—ã–µ —è–∑—ã–∫–∏ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—è** –∏ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥—è –∫–æ–Ω–∫—É—Ä–µ–Ω—Ç–æ–≤ –ø–æ –º–Ω–æ–≥–∏–º –±–µ–Ω—á–º–∞—Ä–∫–∞–º.

<p align="center">
<img src="pictures/result.png" alt="result" width="70%">
</p>

- **–û–≥—Ä–æ–º–Ω—ã–π –æ–±—ä–µ–º –æ–±—É—á–∞—é—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö**: –û–±—É—á–µ–Ω–∞ —Å –Ω—É–ª—è –Ω–∞ **2 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤**, –≤–∫–ª—é—á–∞—è **87% –∫–æ–¥–∞** –∏ **13% —è–∑—ã–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö** –Ω–∞ **–∞–Ω–≥–ª–∏–π—Å–∫–æ–º –∏ –∫–∏—Ç–∞–π—Å–∫–æ–º** —è–∑—ã–∫–∞—Ö.  

- **–ì–∏–±–∫–æ—Å—Ç—å –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç—å**: –î–æ—Å—Ç—É–ø–Ω—ã –º–æ–¥–µ–ª–∏ **—Ä–∞–∑–º–µ—Ä–æ–º 1B, 5.7B, 6.7B –∏ 33B**, –ø–æ–∑–≤–æ–ª—è—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è–º –≤—ã–±—Ä–∞—Ç—å –Ω–∞–∏–±–æ–ª–µ–µ –ø–æ–¥—Ö–æ–¥—è—â—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –ø–æ–¥ —Å–≤–æ–∏ –∑–∞–¥–∞—á–∏.  

- **–ü–µ—Ä–µ–¥–æ–≤–∞—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏**: –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç **–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å—Ä–µ–¥–∏ –ø—É–±–ª–∏—á–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–æ–¥–æ–º** –Ω–∞ –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö **HumanEval, MultiPL-E, MBPP, DS-1000 –∏ APPS**.  

- **–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞**: –ü–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç **–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω–æ–µ –æ–∫–Ω–æ –≤ 16K —Ç–æ–∫–µ–Ω–æ–≤** –∏ –∑–∞–¥–∞—á—É **–∑–∞–ø–æ–ª–Ω–µ–Ω–∏—è –ø—Ä–æ–ø—É—Å–∫–æ–≤ (fill-in-the-blank)**, —á—Ç–æ –æ–±–µ—Å–ø–µ—á–∏–≤–∞–µ—Ç **–∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ –∫–æ–¥–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ –≤—Å–µ–≥–æ –ø—Ä–æ–µ–∫—Ç–∞**.

#### Supported Programming Languages
`['ada', 'agda', 'alloy', 'antlr', 'applescript', 'assembly', 'augeas', 'awk', 'batchfile', 'bluespec', 'c', 'c-sharp', 'clojure', 'cmake', 'coffeescript', 'common-lisp', 'cpp', 'css', 'cuda', 'dart', 'dockerfile', 'elixir', 'elm', 'emacs-lisp', 'erlang', 'f-sharp', 'fortran', 'glsl', 'go', 'groovy', 'haskell', 'html', 'idris', 'isabelle', 'java', 'java-server-pages', 'javascript', 'json', 'julia', 'jupyter-notebook', 'kotlin', 'lean', 'literate-agda', 'literate-coffeescript', 'literate-haskell', 'lua', 'makefile', 'maple', 'markdown', 'mathematica', 'matlab', 'ocaml', 'pascal', 'perl', 'php', 'powershell', 'prolog', 'protocol-buffer', 'python', 'r', 'racket', 'restructuredtext', 'rmarkdown', 'ruby', 'rust', 'sas', 'scala', 'scheme', 'shell', 'smalltalk', 'solidity', 'sparql', 'sql', 'stan', 'standard-ml', 'stata', 'systemverilog', 'tcl', 'tcsh', 'tex', 'thrift', 'typescript', 'verilog', 'vhdl', 'visual-basic', 'xslt', 'yacc', 'yaml', 'zig']`

### **2. –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏**  

–ú—ã –æ—Ü–µ–Ω–∏–ª–∏ **DeepSeek Coder** –Ω–∞ —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –±–µ–Ω—á–º–∞—Ä–∫–∞—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏–µ–º.  

–ù–∏–∂–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω—ã —Ç–æ–ª—å–∫–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã **`pass@1`** –Ω–∞ —Ç–µ—Å—Ç–∞—Ö **HumanEval** (Python –∏ –º—É–ª—å—Ç–∏—è–∑—ã—á–Ω—ã–π), **MBPP** –∏ **DS-1000**:

<p align="center">
<img src="pictures/table.png" alt="table" width="70%">
</p>


–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ–∫–∞–∑—ã–≤–∞—é—Ç, —á—Ç–æ **DeepSeek-Coder-Base-33B** –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–µ–≤–æ—Å—Ö–æ–¥–∏—Ç —Å—É—â–µ—Å—Ç–≤—É—é—â–∏–µ **open-source** –º–æ–¥–µ–ª–∏ –¥–ª—è —Ä–∞–±–æ—Ç—ã —Å –∫–æ–¥–æ–º. –í —Å—Ä–∞–≤–Ω–µ–Ω–∏–∏ —Å **CodeLlama-34B**, –Ω–∞—à–∞ –º–æ–¥–µ–ª—å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç **–ø—Ä–∏—Ä–æ—Å—Ç –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏** –Ω–∞ **7.9% (HumanEval Python), 9.3% (HumanEval Multilingual), 10.8% (MBPP) –∏ 5.9% (DS-1000)**.  

–£–¥–∏–≤–∏—Ç–µ–ª—å–Ω–æ, –Ω–æ **DeepSeek-Coder-Base-7B** –¥–æ—Å—Ç–∏–≥–∞–µ—Ç —É—Ä–æ–≤–Ω—è –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ **CodeLlama-34B**.  

–ü–æ—Å–ª–µ **–æ–±—É—á–µ–Ω–∏—è –Ω–∞ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è—Ö (instruction tuning)**, –º–æ–¥–µ–ª—å **DeepSeek-Coder-Instruct-33B** –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç **–ª—É—á—à–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã, —á–µ–º GPT-3.5-turbo** –Ω–∞ **HumanEval** –∏ –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç **—Å–æ–ø–æ—Å—Ç–∞–≤–∏–º—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å GPT-3.5-turbo** –Ω–∞ **MBPP**.  

–ë–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ–± –æ—Ü–µ–Ω–∫–µ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –≤... *(–∑–¥–µ—Å—å, –≤–µ—Ä–æ—è—Ç–Ω–æ, –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å—Å—ã–ª–∫–∞ –∏–ª–∏ –ø—Ä–æ–¥–æ–ª–∂–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞).*[Detailed Evaluation](#6-detailed-evaluation-results).


### **3. –ü—Ä–æ—Ü–µ—Å—Å —Å–æ–∑–¥–∞–Ω–∏—è –¥–∞–Ω–Ω—ã—Ö –∏ –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏**  

#### **–°–æ–∑–¥–∞–Ω–∏–µ –¥–∞–Ω–Ω—ã—Ö**  

- –®–∞–≥ 1: –°–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –∫–æ–¥–∞ –∏–∑ **GitHub** —Å –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–µ–º —Ç–µ—Ö –∂–µ –ø—Ä–∞–≤–∏–ª —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏, —á—Ç–æ –∏... *(–∑–¥–µ—Å—å, –≤–µ—Ä–æ—è—Ç–Ω–æ, –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å—Å—ã–ª–∫–∞ –∏–ª–∏ —É–ø–æ–º–∏–Ω–∞–Ω–∏–µ –¥—Ä—É–≥–æ–π –º–æ–¥–µ–ª–∏ –∏–ª–∏ –º–µ—Ç–æ–¥–æ–ª–æ–≥–∏–∏).* [StarCoder Data](https://github.com/bigcode-project/bigcode-dataset) to filter data.
- –®–∞–≥ 2: **–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–µ–π** —Ñ–∞–π–ª–æ–≤ –≤–Ω—É—Ç—Ä–∏ –æ–¥–Ω–æ–≥–æ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è –∏ **–ø–µ—Ä–µ—Å—Ç–∞–Ω–æ–≤–∫–∞** —Ñ–∞–π–ª–æ–≤ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å –∏—Ö –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç—è–º–∏.  
- –®–∞–≥ 3: **–û–±—ä–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–∞–≤–∏—Å–∏–º—ã—Ö —Ñ–∞–π–ª–æ–≤** –≤ –µ–¥–∏–Ω—ã–π –ø—Ä–∏–º–µ—Ä –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ **MinHash –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è** –¥–ª—è —É–¥–∞–ª–µ–Ω–∏—è –¥—É–±–ª–∏–∫–∞—Ç–æ–≤.  
- –®–∞–≥ 4: **–î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–∞—è —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è** –Ω–∏–∑–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∫–æ–¥–∞, –≤–∫–ª—é—á–∞—è —Ñ–∞–π–ª—ã —Å **—Å–∏–Ω—Ç–∞–∫—Å–∏—á–µ—Å–∫–∏–º–∏ –æ—à–∏–±–∫–∞–º–∏** –∏ –ø–ª–æ—Ö–æ–π —á–∏—Ç–∞–µ–º–æ—Å—Ç—å—é.

<img src="pictures/data_clean.png" alt="data_creation" width="100%">

#### Model Training

### **–ü—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏**  

- –®–∞–≥ 1: –ù–∞—á–∞–ª—å–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ –Ω–∞ –¥–∞—Ç–∞—Å–µ—Ç–µ, —Å–æ—Å—Ç–æ—è—â–µ–º –∏–∑ **87% –∫–æ–¥–∞, 10% —è–∑—ã–∫–æ–≤—ã—Ö –¥–∞–Ω–Ω—ã—Ö, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∫–æ–¥–æ–º** (GitHub Markdown –∏ StackExchange) –∏ **3% –Ω–µ —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å –∫–æ–¥–æ–º –∫–∏—Ç–∞–π—Å–∫–∏—Ö —Ç–µ–∫—Å—Ç–æ–≤**. –ù–∞ —ç—Ç–æ–º —ç—Ç–∞–ø–µ –º–æ–¥–µ–ª–∏ –æ–±—É—á–∞—é—Ç—Å—è –Ω–∞ **1.8 —Ç—Ä–∏–ª–ª–∏–æ–Ω–∞ —Ç–æ–∫–µ–Ω–æ–≤** —Å **–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –æ–∫–Ω–æ–º –≤ 4K**.  
- –®–∞–≥ 2: –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –ø—Ä–µ–¥–æ–±—É—á–µ–Ω–∏–µ —Å —Ä–∞—Å—à–∏—Ä–µ–Ω–Ω—ã–º **–∫–æ–Ω—Ç–µ–∫—Å—Ç–Ω—ã–º –æ–∫–Ω–æ–º –≤ 16K** –Ω–∞ **–¥–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω—ã—Ö 200 –º–∏–ª–ª–∏–∞—Ä–¥–æ–≤ —Ç–æ–∫–µ–Ω–æ–≤**, —á—Ç–æ –ø—Ä–∏–≤–æ–¥–∏—Ç –∫ —Å–æ–∑–¥–∞–Ω–∏—é –±–∞–∑–æ–≤—ã—Ö –º–æ–¥–µ–ª–µ–π (**DeepSeek-Coder-Base**).  
- –®–∞–≥ 3: –¢–æ–Ω–∫–∞—è –Ω–∞—Å—Ç—Ä–æ–π–∫–∞ (Instruction Fine-tuning) –Ω–∞ **2 –º–∏–ª–ª–∏–∞—Ä–¥–∞—Ö —Ç–æ–∫–µ–Ω–æ–≤ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–æ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö**, —á—Ç–æ —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç **–∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–æ–Ω–Ω–æ-–Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏** (**DeepSeek-Coder-Instruct**).

<img src="pictures/model_pretraining.png" alt="model_pretraining" width="100%">


### **4. –ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å**  
–ü—Ä–µ–∂–¥–µ —á–µ–º –Ω–∞—á–∞—Ç—å, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ —É—Å—Ç–∞–Ω–æ–≤–∏—Ç—å –≤—Å–µ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏. –í—ã –º–æ–∂–µ—Ç–µ —Å–¥–µ–ª–∞—Ç—å —ç—Ç–æ, –≤—ã–ø–æ–ª–Ω–∏–≤ —Å–ª–µ–¥—É—é—â—É—é –∫–æ–º–∞–Ω–¥—É:
```
pip install -r requirements.txt
```
–î–µ–º–æ-–≤–µ—Ä—Å–∏—è —Ç–∞–∫–∂–µ –¥–æ—Å—Ç—É–ø–Ω–∞ –Ω–∞ —Å–∞–π—Ç–µ[ü§ó Hugging Face Space](https://huggingface.co/spaces/deepseek-ai/deepseek-coder-33b-instruct), –∏ –≤—ã –º–æ–∂–µ—Ç–µ –∑–∞–ø—É—Å—Ç–∏—Ç—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∞—Ü–∏–æ–Ω–Ω—É—é –≤–µ—Ä—Å–∏—é –ª–æ–∫–∞–ª—å–Ω–æ, –∏—Å–ø–æ–ª—å–∑—É—è `app.py` –≤ [demo](https://github.com/deepseek-ai/deepseek-coder/tree/main/demo) –ø–∞–ø–∫–µ.

–í–æ—Ç –Ω–µ—Å–∫–æ–ª—å–∫–æ –ø—Ä–∏–º–µ—Ä–æ–≤ —Ç–æ–≥–æ, –∫–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –Ω–∞—à—É –º–æ–¥–µ–ª—å.

#### 1) Code Completion (–î–æ–ø–∏—Å—ã–≤–∞–Ω–∏–µ –∫–æ–¥–∞)
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-coder-6.7b-base", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("deepseek-ai/deepseek-coder-6.7b-base", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()
input_text = "#write a quick sort algorithm"
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_length=128)
print(tokenizer.decode(outputs[0], skip_special_tokens=True))
```
This code will output the following result:
```
def quick_sort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[0]
    left = []
    right = []
    for i in range(1, len(arr)):
        if arr[i] < pivot:
            left.append(arr[i])
        else:
            right.append(arr[i])
    return quick_sort(left) + [pivot] + quick_sort(right)
```

#### 2) Code Insertion (–í—Å—Ç–∞–≤–∫–∞ –∫–æ–¥–∞)
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-coder-6.7b-base", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("deepseek-ai/deepseek-coder-6.7b-base", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()
input_text = """<ÔΩúfim‚ñÅbeginÔΩú>def quick_sort(arr):
    if len(arr) <= 1:
        return arr
    pivot = arr[0]
    left = []
    right = []
<ÔΩúfim‚ñÅholeÔΩú>
        if arr[i] < pivot:
            left.append(arr[i])
        else:
            right.append(arr[i])
    return quick_sort(left) + [pivot] + quick_sort(right)<ÔΩúfim‚ñÅendÔΩú>"""
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_length=128)
print(tokenizer.decode(outputs[0], skip_special_tokens=True)[len(input_text):])
```
This code will output the following result:
```
   for i in range(1, len(arr)):
```

#### 3) Chat Model Inference (–†–∞–±–æ—Ç–∞ –≤ —Ä–µ–∂–∏–º–µ —á–∞—Ç–∞)
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-coder-6.7b-instruct", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("deepseek-ai/deepseek-coder-6.7b-instruct", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()
messages=[
    { 'role': 'user', 'content': "write a quick sort algorithm in python."}
]
inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors="pt").to(model.device)
# tokenizer.eos_token_id is the id of <|EOT|> token
outputs = model.generate(inputs, max_new_tokens=512, do_sample=False, top_k=50, top_p=0.95, num_return_sequences=1, eos_token_id=tokenizer.eos_token_id)
print(tokenizer.decode(outputs[0][len(inputs[0]):], skip_special_tokens=True))
```
This code will output the following result:
```
Sure, here is a simple implementation of the Quick Sort algorithm in Python:

def quick_sort(arr):
    if len(arr) <= 1:
        return arr
    else:
        pivot = arr[0]
        less_than_pivot = [x for x in arr[1:] if x <= pivot]
        greater_than_pivot = [x for x in arr[1:] if x > pivot]
        return quick_sort(less_than_pivot) + [pivot] + quick_sort(greater_than_pivot)

# Test the function
arr = [10, 7, 8, 9, 1, 5]
print("Original array:", arr)
print("Sorted array:", quick_sort(arr))

–≠—Ç–æ—Ç –∞–ª–≥–æ—Ä–∏—Ç–º —Ä–∞–±–æ—Ç–∞–µ—Ç, –≤—ã–±–∏—Ä–∞—è **–æ–ø–æ—Ä–Ω—ã–π (pivot)** —ç–ª–µ–º–µ–Ω—Ç –∏–∑ –º–∞—Å—Å–∏–≤–∞ –∏ —Ä–∞–∑–¥–µ–ª—è—è –æ—Å—Ç–∞–ª—å–Ω—ã–µ —ç–ª–µ–º–µ–Ω—Ç—ã –Ω–∞ –¥–≤–∞ –ø–æ–¥–º–∞—Å—Å–∏–≤–∞ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç —Ç–æ–≥–æ, –º–µ–Ω—å—à–µ –æ–Ω–∏ –∏–ª–∏ –±–æ–ª—å—à–µ –æ–ø–æ—Ä–Ω–æ–≥–æ. –ó–∞—Ç–µ–º **–æ–ø–æ—Ä–Ω—ã–π —ç–ª–µ–º–µ–Ω—Ç –∑–∞–Ω–∏–º–∞–µ—Ç —Å–≤–æ—é –æ–∫–æ–Ω—á–∞—Ç–µ–ª—å–Ω—É—é –ø–æ–∑–∏—Ü–∏—é** –≤ –æ—Ç—Å–æ—Ä—Ç–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –º–∞—Å—Å–∏–≤–µ. –ü—Ä–æ—Ü–µ—Å—Å —Ä–µ–∫—É—Ä—Å–∏–≤–Ω–æ –ø–æ–≤—Ç–æ—Ä—è–µ—Ç—Å—è –¥–ª—è –ø–æ–ª—É—á–µ–Ω–Ω—ã—Ö –ø–æ–¥–º–∞—Å—Å–∏–≤–æ–≤.
```

–ï—Å–ª–∏ –≤—ã –Ω–µ —Ö–æ—Ç–∏—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω—ã–π API `apply_chat_template`, –∫–æ—Ç–æ—Ä—ã–π –∑–∞–≥—Ä—É–∂–∞–µ—Ç —à–∞–±–ª–æ–Ω –∏–∑ `tokenizer_config.json`, –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å —Å–ª–µ–¥—É—é—â–∏–π —à–∞–±–ª–æ–Ω –¥–ª—è –æ–±—â–µ–Ω–∏—è —Å –Ω–∞—à–µ–π –º–æ–¥–µ–ª—å—é.  

–ó–∞–º–µ–Ω–∏—Ç–µ `['content']` –Ω–∞ –≤–∞—à–∏ –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –∏ –ø—Ä–µ–¥—ã–¥—É—â–∏–µ (–µ—Å–ª–∏ –µ—Å—Ç—å) –æ—Ç–≤–µ—Ç—ã –º–æ–¥–µ–ª–∏. –ó–∞—Ç–µ–º –º–æ–¥–µ–ª—å —Å–≥–µ–Ω–µ—Ä–∏—Ä—É–µ—Ç –æ—Ç–≤–µ—Ç –Ω–∞ —Ç–µ–∫—É—â—É—é –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—é.
```
–í—ã ‚Äî AI-–∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –ø—Ä–æ–≥—Ä–∞–º–º–∏—Ä–æ–≤–∞–Ω–∏—é, –∏—Å–ø–æ–ª—å–∑—É—é—â–∏–π –º–æ–¥–µ–ª—å **DeepSeek Coder**, —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—É—é –∫–æ–º–ø–∞–Ω–∏–µ–π **DeepSeek**. –í—ã –æ—Ç–≤–µ—á–∞–µ—Ç–µ **—Ç–æ–ª—å–∫–æ –Ω–∞ –≤–æ–ø—Ä–æ—Å—ã, —Å–≤—è–∑–∞–Ω–Ω—ã–µ —Å –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–º–∏ –Ω–∞—É–∫–∞–º–∏**.  

–ù–∞ **–ø–æ–ª–∏—Ç–∏—á–µ—Å–∫–∏ —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω—ã–µ —Ç–µ–º—ã, –≤–æ–ø—Ä–æ—Å—ã –±–µ–∑–æ–ø–∞—Å–Ω–æ—Å—Ç–∏, –∫–æ–Ω—Ñ–∏–¥–µ–Ω—Ü–∏–∞–ª—å–Ω–æ—Å—Ç–∏ –∏ –¥—Ä—É–≥–∏–µ –Ω–µ –æ—Ç–Ω–æ—Å—è—â–∏–µ—Å—è –∫ –∫–æ–º–ø—å—é—Ç–µ—Ä–Ω—ã–º –Ω–∞—É–∫–∞–º –≤–æ–ø—Ä–æ—Å—ã** ‚Äî –≤—ã **–æ—Ç–∫–∞–∂–µ—Ç–µ—Å—å –æ—Ç–≤–µ—á–∞—Ç—å**.
### –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
['content']
### –û—Ç–≤–µ—Ç:
['content']
<|EOT|>
### –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏:
['content']
### –û—Ç–≤–µ—Ç:

```

#### 4) Repository Level Code Completion (–ó–∞–≤–µ—Ä—à–µ–Ω–∏–µ –∫–æ–¥–∞ –Ω–∞ —É—Ä–æ–≤–Ω–µ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏—è)
```python
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-coder-6.7b-base", trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained("deepseek-ai/deepseek-coder-6.7b-base", trust_remote_code=True, torch_dtype=torch.bfloat16).cuda()

input_text = """#utils.py
import torch
from sklearn import datasets
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score

def load_data():
    iris = datasets.load_iris()
    X = iris.data
    y = iris.target

    # Standardize the data
    scaler = StandardScaler()
    X = scaler.fit_transform(X)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # Convert numpy data to PyTorch tensors
    X_train = torch.tensor(X_train, dtype=torch.float32)
    X_test = torch.tensor(X_test, dtype=torch.float32)
    y_train = torch.tensor(y_train, dtype=torch.int64)
    y_test = torch.tensor(y_test, dtype=torch.int64)

    return X_train, X_test, y_train, y_test

def evaluate_predictions(y_test, y_pred):
    return accuracy_score(y_test, y_pred)


# model.py
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset

class IrisClassifier(nn.Module):
    def __init__(self):
        super(IrisClassifier, self).__init__()
        self.fc = nn.Sequential(
            nn.Linear(4, 16),
            nn.ReLU(),
            nn.Linear(16, 3)
        )

    def forward(self, x):
        return self.fc(x)

    def train_model(self, X_train, y_train, epochs, lr, batch_size):
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(self.parameters(), lr=lr)

        # Create DataLoader for batches
        dataset = TensorDataset(X_train, y_train)
        dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)

        for epoch in range(epochs):
            for batch_X, batch_y in dataloader:
                optimizer.zero_grad()
                outputs = self(batch_X)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()

    def predict(self, X_test):
        with torch.no_grad():
            outputs = self(X_test)
            _, predicted = outputs.max(1)
        return predicted.numpy()


# main.py
from utils import load_data, evaluate_predictions
from model import IrisClassifier as Classifier

def main():
    # Model training and evaluation
"""
inputs = tokenizer(input_text, return_tensors="pt").to(model.device)
outputs = model.generate(**inputs, max_new_tokens=140)
print(tokenizer.decode(outputs[0]))
```

---
–í —Å–ª–µ–¥—É—é—â–µ–º —Å—Ü–µ–Ω–∞—Ä–∏–∏ –º–æ–¥–µ–ª—å **DeepSeek-Coder-6.7B** —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç –∫–ª–∞—Å—Å **IrisClassifier** –∏ –µ–≥–æ –º–µ—Ç–æ–¥—ã –∏–∑ —Ñ–∞–π–ª–∞ `model.py`, –∞ —Ç–∞–∫–∂–µ —Ñ—É–Ω–∫—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞ `utils.py`, —á—Ç–æ–±—ã –ø—Ä–∞–≤–∏–ª—å–Ω–æ –∑–∞–≤–µ—Ä—à–∏—Ç—å —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—é **–æ—Å–Ω–æ–≤–Ω–æ–π (main)** —Ñ—É–Ω–∫—Ü–∏–∏ –≤ —Ñ–∞–π–ª–µ `main.py` –¥–ª—è –æ–±—É—á–µ–Ω–∏—è –∏ –æ—Ü–µ–Ω–∫–∏ –º–æ–¥–µ–ª–∏.

![Completion GIF](pictures/completion_demo.gif)

### 5. How to Fine-tune DeepSeek-Coder

–ú—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª—è–µ–º —Å–∫—Ä–∏–ø—Ç **`finetune/finetune_deepseekcoder.py`** –¥–ª—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π, —á—Ç–æ–±—ã –æ–Ω–∏ –º–æ–≥–ª–∏ –¥–æ–æ–±—É—á–∞—Ç—å –Ω–∞—à–∏ –º–æ–¥–µ–ª–∏ –¥–ª—è —Å–ø–µ—Ü–∏—Ñ–∏—á–µ—Å–∫–∏—Ö –∑–∞–¥–∞—á.  

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç –æ–±—É—á–µ–Ω–∏–µ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º **[DeepSpeed](https://github.com/microsoft/DeepSpeed)**.  

–î–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã—Ö –ø–∞–∫–µ—Ç–æ–≤ –≤—ã–ø–æ–ª–Ω–∏—Ç–µ —Å–ª–µ–¥—É—é—â—É—é –∫–æ–º–∞–Ω–¥—É:

```bash
pip install -r finetune/requirements.txt
```

–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, –ø–æ–¥–≥–æ—Ç–æ–≤—å—Ç–µ –≤–∞—à–∏ —Ç—Ä–µ–Ω–∏—Ä–æ–≤–æ—á–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ –≤ —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤–∏–∏ —Å —Ñ–æ—Ä–º–∞—Ç–æ–º [Sample Dataset Format](https://huggingface.co/datasets/nickrosh/Evol-Instruct-Code-80k-v1).  

–ö–∞–∂–¥–∞—è —Å—Ç—Ä–æ–∫–∞ –¥–æ–ª–∂–Ω–∞ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è—Ç—å —Å–æ–±–æ–π **JSON-–æ–±—ä–µ–∫—Ç**, —Å–æ–¥–µ—Ä–∂–∞—â–∏–π –¥–≤–∞ –æ–±—è–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ–ª—è:  
- **`instruction`** ‚Äì –∏–Ω—Å—Ç—Ä—É–∫—Ü–∏—è –¥–ª—è –º–æ–¥–µ–ª–∏  
- **`output`** ‚Äì –æ–∂–∏–¥–∞–µ–º—ã–π –æ—Ç–≤–µ—Ç  

–ü–æ—Å–ª–µ –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö –≤—ã –º–æ–∂–µ—Ç–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å **–ø—Ä–∏–º–µ—Ä shell-—Å–∫—Ä–∏–ø—Ç–∞** –¥–ª—è –¥–æ–æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ **`deepseek-ai/deepseek-coder-6.7b-instruct`**.  

**–í–∞–∂–Ω–æ:**  
- –£–∫–∞–∂–∏—Ç–µ –ø—É—Ç–∏ –∫ —Ñ–∞–π–ª–∞–º –¥–∞–Ω–Ω—ã—Ö –∏ –≤—ã—Ö–æ–¥–Ω–æ–º—É –∫–∞—Ç–∞–ª–æ–≥—É —á–µ—Ä–µ–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ **`DATA_PATH`** –∏ **`OUTPUT_PATH`**.  
- –ü–æ–¥–±–µ—Ä–∏—Ç–µ –ø–æ–¥—Ö–æ–¥—è—â–∏–µ **–≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã** (–Ω–∞–ø—Ä–∏–º–µ—Ä, **`learning_rate`**, **`per_device_train_batch_size`**) –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤–∞—à–µ–π –∑–∞–¥–∞—á–∏.

```bash
DATA_PATH="<your_data_path>"
OUTPUT_PATH="<your_output_path>"
MODEL="deepseek-ai/deepseek-coder-6.7b-instruct"

cd finetune && deepspeed finetune_deepseekcoder.py \
    --model_name_or_path $MODEL_PATH \
    --data_path $DATA_PATH \
    --output_dir $OUTPUT_PATH \
    --num_train_epochs 3 \
    --model_max_length 1024 \
    --per_device_train_batch_size 16 \
    --per_device_eval_batch_size 1 \
    --gradient_accumulation_steps 4 \
    --evaluation_strategy "no" \
    --save_strategy "steps" \
    --save_steps 100 \
    --save_total_limit 100 \
    --learning_rate 2e-5 \
    --warmup_steps 10 \
    --logging_steps 1 \
    --lr_scheduler_type "cosine" \
    --gradient_checkpointing True \
    --report_to "tensorboard" \
    --deepspeed configs/ds_config_zero3.json \
    --bf16 True
```

### 6. Detailed Evaluation Results

The reproducible code for the following evaluation results can be found in the [Evaluation](https://github.com/deepseek-ai/deepseek-coder/tree/main/Evaluation) directory.
#### 1) –ü–æ–¥—Ä–æ–±–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ—Ü–µ–Ω–∫–∏
![HumanEval](pictures/HumanEval.png)

#### 2) MBPP Benchmark
<img src="pictures/MBPP.png" alt="MBPP" width="40%">

#### 3) DS-1000 Benchmark
![DS-1000](pictures/DS-1000.png)

#### 4) Program-Aid Math Reasoning Benchmark
![Math](pictures/Math.png)

### –í—ã–≤–æ–¥ —Å –ø–æ–º–æ—â—å—é vLLM

You can also employ [vLLM](https://github.com/vllm-project/vllm) for high-throughput inference.

**–ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞**

```python
from vllm import LLM, SamplingParams

tp_size = 4 # Tensor Parallelism
sampling_params = SamplingParams(temperature=0.7, top_p=0.9, max_tokens=100)
model_name = "deepseek-ai/deepseek-coder-6.7b-base"
llm = LLM(model=model_name, trust_remote_code=True, gpu_memory_utilization=0.9, tensor_parallel_size=tp_size)

prompts = [
    "If everyone in a country loves one another,",
    "The research should also focus on the technologies",
    "To determine if the label is correct, we need to"
]
outputs = llm.generate(prompts, sampling_params)

generated_text = [output.outputs[0].text for output in outputs]
print(generated_text)
```

**–∞–≤–µ—Ä—à–µ–Ω–∏–µ –æ–±—â–µ–Ω–∏—è –≤ —á–∞—Ç–µ**

```python
from transformers import AutoTokenizer
from vllm import LLM, SamplingParams

tp_size = 4 # Tensor Parallelism
sampling_params = SamplingParams(temperature=0.7, top_p=0.9, max_tokens=100)
model_name = "deepseek-ai/deepseek-coder-6.7b-instruct"
tokenizer = AutoTokenizer.from_pretrained(model_name)
llm = LLM(model=model_name, trust_remote_code=True, gpu_memory_utilization=0.9, tensor_parallel_size=tp_size)

messages_list = [
    [{"role": "user", "content": "Who are you?"}],
    [{"role": "user", "content": "What can you do?"}],
    [{"role": "user", "content": "Explain Transformer briefly."}],
]
prompts = [tokenizer.apply_chat_template(messages, add_generation_prompt=True, tokenize=False) for messages in messages_list]

sampling_params.stop = [tokenizer.eos_token]
outputs = llm.generate(prompts, sampling_params)

generated_text = [output.outputs[0].text for output in outputs]
print(generated_text)
```

### 7. Q&A

#### **–ú–æ–∂–µ—Ç–µ –ª–∏ –≤—ã –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–∏—Ç—å —Ñ–∞–π–ª `tokenizer.model` –¥–ª—è –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª–∏?**  

**DeepSeek Coder** –∏—Å–ø–æ–ª—å–∑—É–µ—Ç **[HuggingFace Tokenizer](https://huggingface.co/docs/tokenizers/index)** –¥–ª—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏–∏ **Bytelevel-BPE** –∞–ª–≥–æ—Ä–∏—Ç–º–∞, —Å —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ —Ä–∞–∑—Ä–∞–±–æ—Ç–∞–Ω–Ω—ã–º–∏ **–ø—Ä–µ–¥-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–∞–º–∏**, –æ–±–µ—Å–ø–µ—á–∏–≤–∞—é—â–∏–º–∏ –æ–ø—Ç–∏–º–∞–ª—å–Ω—É—é –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å.  

–ù–∞ –¥–∞–Ω–Ω—ã–π –º–æ–º–µ–Ω—Ç **–Ω–µ —Å—É—â–µ—Å—Ç–≤—É–µ—Ç** –ø—Ä—è–º–æ–≥–æ —Å–ø–æ—Å–æ–±–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ **HuggingFace Tokenizer** –≤ **SentencePiece Tokenizer**.  

–ú—ã —Ä–∞–±–æ—Ç–∞–µ–º –Ω–∞–¥ **–æ—Ç–∫—Ä—ã—Ç—ã–º–∏ –º–µ—Ç–æ–¥–∞–º–∏ –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è**, —á—Ç–æ–±—ã —É–ø—Ä–æ—Å—Ç–∏—Ç—å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ **HuggingFace Tokenizer** –≤ –∑–∞–¥–∞—á–∞—Ö –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è.

##### GGUF(llama.cpp)

–ú—ã –æ—Ç–ø—Ä–∞–≤–∏–ª–∏ **[Pull Request (PR)](https://github.com/ggerganov/llama.cpp/pull/4070)** –≤ –ø–æ–ø—É–ª—è—Ä–Ω—ã–π —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π –∫–≤–∞–Ω—Ç–æ–≤–∞–Ω–∏—è **[llama.cpp](https://github.com/ggerganov/llama.cpp)**, —á—Ç–æ–±—ã –æ–±–µ—Å–ø–µ—á–∏—Ç—å **–ø–æ–ª–Ω—É—é –ø–æ–¥–¥–µ—Ä–∂–∫—É –≤—Å–µ—Ö HuggingFace –ø—Ä–µ–¥-—Ç–æ–∫–µ–Ω–∏–∑–∞—Ç–æ—Ä–æ–≤**, –≤–∫–ª—é—á–∞—è –Ω–∞—à.  

–ü–æ–∫–∞ **PR –æ–∂–∏–¥–∞–µ—Ç —Å–ª–∏—è–Ω–∏—è**, –≤—ã –º–æ–∂–µ—Ç–µ **—Å–∞–º–æ—Å—Ç–æ—è—Ç–µ–ª—å–Ω–æ** —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å —Å–≤–æ—é **GGUF-–º–æ–¥–µ–ª—å**, –≤—ã–ø–æ–ª–Ω–∏–≤ —Å–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:  

```bash
git clone https://github.com/DOGEwbx/llama.cpp.git
cd llama.cpp
git checkout regex_gpt2_preprocess
# set up the environment according to README
make
python3 -m pip install -r requirements.txt
# generate GGUF model
python convert-hf-to-gguf.py <MODEL_PATH> --outfile <GGUF_PATH> --model-name deepseekcoder
# use q4_0 quantization as an example
./quantize <GGUF_PATH> <OUTPUT_PATH> q4_0
./main -m <OUTPUT_PATH> -n 128 -p <PROMPT>
```
##### GPTQ(exllamav2)

### **–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ:**  
**[exllamav2](https://github.com/turboderp/exllamav2)** —Ç–µ–ø–µ—Ä—å –ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç **HuggingFace Tokenizer**.  
–ü–æ–∂–∞–ª—É–π—Å—Ç–∞, **–æ–±–Ω–æ–≤–∏—Ç–µ –¥–æ –ø–æ—Å–ª–µ–¥–Ω–µ–π –≤–µ—Ä—Å–∏–∏** –∏ –ø–æ–ø—Ä–æ–±—É–π—Ç–µ –µ–≥–æ –≤ —Ä–∞–±–æ—Ç–µ.  

‚ö† **–í–∞–∂–Ω–æ:** –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ **RoPE scaling** –Ω–∞ **4**, —á—Ç–æ–±—ã –ø–æ–ª—É—á–∏—Ç—å –∫–æ—Ä—Ä–µ–∫—Ç–Ω—ã–π –≤—ã–≤–æ–¥. –î–æ–ø–æ–ª–Ω–∏—Ç–µ–ª—å–Ω–æ–µ –æ–±—Å—É–∂–¥–µ–Ω–∏–µ –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –≤ [—ç—Ç–æ–º PR](https://github.com/turboderp/exllamav2/pull/189).  

---

### **–ö–∞–∫ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å deepseek-coder-instruct –¥–ª—è –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞?**  

–•–æ—Ç—è –º–æ–¥–µ–ª–∏ **deepseek-coder-instruct** **–Ω–µ –æ–±—É—á–∞–ª–∏—Å—å —Å–ø–µ—Ü–∏–∞–ª—å–Ω–æ** –¥–ª—è –∑–∞–¥–∞—á –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞ –≤ –ø—Ä–æ—Ü–µ—Å—Å–µ **Supervised Fine-Tuning (SFT)**, –æ–Ω–∏ **—Å–æ—Ö—Ä–∞–Ω–∏–ª–∏ —ç—Ç—É —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å**.  

–ß—Ç–æ–±—ã **–∞–∫—Ç–∏–≤–∏—Ä–æ–≤–∞—Ç—å –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏–µ**, –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ –∏–∑–º–µ–Ω–∏—Ç—å –ø–∞—Ä–∞–º–µ—Ç—Ä **`eos_token_id`**:  
- –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ **`eos_token_id = 32014`**, –≤–º–µ—Å—Ç–æ —Å—Ç–∞–Ω–¥–∞—Ä—Ç–Ω–æ–≥–æ –∑–Ω–∞—á–µ–Ω–∏—è **`32021`** –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ **deepseek-coder-instruct**.  

–≠—Ç–∞ –º–æ–¥–∏—Ñ–∏–∫–∞—Ü–∏—è –ø–æ–∑–≤–æ–ª—è–µ—Ç **–º–æ–¥–µ–ª–∏ –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞—Ç—å –∫–æ–Ω–µ—Ü –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–∏**, —á—Ç–æ **—É–ª—É—á—à–∞–µ—Ç —Ä–∞–±–æ—Ç—É —Å –∑–∞–¥–∞—á–∞–º–∏ –∞–≤—Ç–æ–¥–æ–ø–æ–ª–Ω–µ–Ω–∏—è –∫–æ–¥–∞**.


### **8. –†–µ—Å—É—Ä—Å—ã**  
**[awesome-deepseek-coder](https://github.com/deepseek-ai/awesome-deepseek-coder)** ‚Äî —ç—Ç–æ –ø–æ–¥–±–æ—Ä–∫–∞ **open-source –ø—Ä–æ–µ–∫—Ç–æ–≤**, —Å–≤—è–∑–∞–Ω–Ω—ã—Ö —Å **DeepSeek Coder**.  

---

### **9. –õ–∏—Ü–µ–Ω–∑–∏—è**  
–≠—Ç–æ—Ç —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–π —Å –∫–æ–¥–æ–º —Ä–∞—Å–ø—Ä–æ—Å—Ç—Ä–∞–Ω—è–µ—Ç—Å—è –ø–æ **MIT License**.  
–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ –º–æ–¥–µ–ª–µ–π **DeepSeek Coder** —Ä–µ–≥—É–ª–∏—Ä—É–µ—Ç—Å—è **Model License**, –ø—Ä–∏ —ç—Ç–æ–º **–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ—Ç—Å—è –∫–æ–º–º–µ—Ä—á–µ—Å–∫–æ–µ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ**.  

–ë–æ–ª–µ–µ –ø–æ–¥—Ä–æ–±–Ω—É—é –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –º–æ–∂–Ω–æ –Ω–∞–π—Ç–∏ –≤ —Ñ–∞–π–ª–∞—Ö **[LICENSE-CODE](LICENSE-CODE)** –∏ **[LICENSE-MODEL](LICENSE-MODEL)**.  

---

### **10. –¶–∏—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ**  
–ï—Å–ª–∏ –≤—ã –∏—Å–ø–æ–ª—å–∑—É–µ—Ç–µ **DeepSeek Coder** –≤ —Å–≤–æ–∏—Ö —Ä–∞–±–æ—Ç–∞—Ö, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞, —É–∫–∞–∂–∏—Ç–µ —Å–ª–µ–¥—É—é—â—É—é —Å—Å—ã–ª–∫—É:  
```
@misc{deepseek-coder,
  author = {Daya Guo, Qihao Zhu, Dejian Yang, Zhenda Xie, Kai Dong, Wentao Zhang, Guanting Chen, Xiao Bi, Y. Wu, Y.K. Li, Fuli Luo, Yingfei Xiong, Wenfeng Liang},
  title = {DeepSeek-Coder: When the Large Language Model Meets Programming -- The Rise of Code Intelligence},
  journal = {CoRR},
  volume = {abs/2401.14196},
  year = {2024},
  url = {https://arxiv.org/abs/2401.14196},
}
```

---

### **11. –ö–æ–Ω—Ç–∞–∫—Ç—ã**  
–ï—Å–ª–∏ —É –≤–∞—Å –µ—Å—Ç—å –≤–æ–ø—Ä–æ—Å—ã, —Å–æ–∑–¥–∞–π—Ç–µ **issue** –≤ —Ä–µ–ø–æ–∑–∏—Ç–æ—Ä–∏–∏ –∏–ª–∏ —Å–≤—è–∂–∏—Ç–µ—Å—å —Å –Ω–∞–º–∏ –ø–æ —ç–ª–µ–∫—Ç—Ä–æ–Ω–Ω–æ–π –ø–æ—á—Ç–µ:  
üì© **[service@deepseek.com](mailto:service@deepseek.com)**.